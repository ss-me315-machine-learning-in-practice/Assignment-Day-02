---
title: "ME315-day02"
output: html_document
---

# 1. Generating random numbers from distributions

The following commands can be used to generate from various distributions

Uniform: `runif(n, min=0, max=1)`

Normal: `rnorm(n, mean = 0, sd = 1)`'

Beta: `rbeta(n, shape1, shape2)`

Gamma / chi-square / Exponential: `rgamma(n, shape, rate = 1)` Note: scale = 1/rate rpois(n, lambda)

Binomial/Bernoulli: `rbinom(n, size, prob)`

Negative Binomial: `rnbinom(n, size, prob)`

Poisson: rpois(n,lambda)

Probability density/mass functions, Cumulative density functions and quantile functions are also available. Check the result of

```{r}
dnorm(0) 
dnorm(0)*sqrt(2*pi)
qnorm(0.975) 
pnorm(1.96)
```


## Monte Carlo calculations in the case of Uniform(0,1)

Generate 20 points from the Uniform(0,1) distribution. Calculate the sample mean, median, variance and provide a 95% interval by computing the 2.5% and 97.5% percentiles. Repeat with 10000 points and compare the results with the true values of the calculated quantities. 

The true values are mean$=0.5$, median$=0.5$, and variance $=1/12=0.08333333$. 

The support is defined by the two parameters, a=0 and b=1, which are its minimum and maximum values.


```{r}
x=runif(10)
summary(x)
var(x)

x=runif(10^4)
summary(x)
var(x)
```


## Activity 1

Repeat with a different distribution.

### Add your code here


# 2. Density estimation

The simplest way to estimate the pdf of a continuous random variable is with a histogram. For example suppose that we are observing a sample of independent random variables from the standard normal N(0,1) distribution. How will the histogram look like?

```{r}
n=100
y=rnorm(n)
hist(y,freq = FALSE)
hist(y,col=2, freq= FALSE)
```

This may look too much like the pdf of the N(0,1) but let's try it again with a bigger sample size and more breaks in the histogram

```{r}
n=10000
y=rnorm(n)
hist(y,col=2,breaks=100,freq = FALSE)
```

It seems a bit better but the problem is that histogram is a step function and we are trying to estimate a smooth curve. Kernel density estimators are more suitable for this task. The command required is `density()`. Below we also overlay the true pdf (purple line) of the N(0,1) which can be obtains by the R command `dnorm()` 

```{r}
n=10000
y=rnorm(n)
d = density(y)
plot(d, main="Kernel Density of y")
polygon(d, col="red", border="blue")
x=seq(-3,3,length=1000)
lines(x,dnorm(x),col='purple')
```

## Activity 2

Repeat with a different distribution

###  Add your code here

# 3. Point Estimation / Confidence Intervals

## Frequentist Estimation and Confidence Intervals 

Suppose that we observe a single observation $y=98$ from the $N(\theta,80)$ distribution. Find the MLE and provide a $95\%$ confidence interval for $\theta$.

Solution:

The MLE of theta is just y=98.

For 95% confidence intervals use the following code
```{r}
y=98
c(y-1.96*sqrt(80)/sqrt(1),y+1.96*sqrt(80)/sqrt(1))

```

## Bayesian Estimation and Credible Intervals

We also conduct Bayesian Inference based on the prior N($110, 120$) which results in the posterior becomes N($102.8, 48$). Provide a Bayes estimator and a 95$\%$ credible interval for $\theta$.

Solution:

Bayes estimators corresponding to posterior mean, median and mode all give mupost$=102.8$.

For a 95$\%$ credible use:
```{r}
mupost = 102.8
sdpost = sqrt(48)
qnorm(c(0.025,0.975), mupost,sdpost )
```

or else with Monte Carlo:

```{r}
N=100000
x=rnorm(N,mupost,sdpost)
quantile(x,probs=c(0.025,0.975))
```

## Activity 3

Suppose that the posterior a parameter $\theta$ is the Beta($5,7$) distribution. Provide a Bayes estimator and a 95$/%$ credible interval for $\theta$.


### Add your code here

# 4. Polynomial curve fitting

Let's do a similar experiment to the polynomial curve fitting in the lecture of day 1. We will define the 'true' data generating process to be 
$$
f(x) = sin(2\pi x)
$$
where $x$ takes values in $[0,1]$

```{r}
x=seq(0,1,length=1000)
f=sin(2*pi*x)
plot(x,f,xlab='x',ylab='f',col='green',type='l')
```

But we are only going to observe the process in $15$ points distributed evenely in the intervals $[0,1]$. Moreover the process will be observed with additional measurement error that will be distributed as N$(0,0.3^2)$. Hence the distribution of each of these observation $y_i$ will be N$\big( f(x_i),0.3^2$)

```{r}
xo=seq(0,1,length=15)
fo=sin(2*pi*xo)
y = rnorm(15,fo,0.3) 
plot(x,f,xlab='x',ylab='f',col='green',type='l')
points(xo,y, col='blue')
```

Consider fitting a polynomial of first order (linear). This can be done with the command `lm()`. After fitting the polynomial we will overlay its predictions across the unseen data x (test dataset) and also calculate the test mean squared error. 

```{r}
fit1 = lm(y~xo)
pol1 <- function(x) fit1$coefficients[2]*x + fit1$coefficients[1]
plot(x,f,xlab='x',ylab='f',col='green',type='l')
lines(x,pol1(x),col='red')
points(xo,y, col='blue')
mse1 = mean((f-pol1(x))^2)
mse1
```

Let's see what happens with a 3rd degree polynomial.

```{r}
fit3 = lm(y~xo+I(xo^2)+I(xo^3))
#fit2$coefficients
pol3 <- function(x) fit3$coefficients[4]*x^3+fit3$coefficients[3]*x^2+fit3$coefficients[2]*x+ fit3$coefficients[1]
plot(x,f,xlab='x',ylab='f',col='green',type='l')
lines(x,pol3(x),col='red')
points(xo,y, col='blue')
mse3 = mean((f-pol3(x))^2)
mse3
```

## Activity 4 (optional)

Consider all polynomials of degree from 1 to 8 and choose the best in terms of test MSE.

### Add your code here